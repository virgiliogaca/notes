---
tags: [ai, llm, papers]
phase: 4
---

# ðŸŸ£ Phase 4 â€“ LLM Architecture & Papers (Weeks 13â€“16)

## Week 13 â€“ Attention Is All You Need

### Read
- [[Attention Is All You Need]]

Focus on:
- diagrams
- architecture
- ignore dense math initially

---

## Week 14 â€“ GPT Architecture

### Read
- [[GPT-2 Paper]]

Concepts:
- [[Decoder Only Models]]
- [[Language Modeling Objective]]

---

## Week 15 â€“ Scaling Laws

### Read
- [[Scaling Laws for Neural Language Models]]

Concepts:
- [[Model Size]]
- [[Data Scaling]]
- [[Compute Tradeoffs]]

---

## Week 16 â€“ Integration

Revisit:
- [[Softmax]]
- [[Attention Equation]]
- [[Transformers]]

Checkpoint:
> LLMs feel engineered, not magical.
