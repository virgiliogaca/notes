---
tags: [ai, transformers, attention]
phase: 3
---

# ðŸ”µ Phase 3 â€“ Attention & Transformers (Weeks 9â€“12)

## Core Equation

[[Attention Equation]]:
`Attention(Q,K,V) = softmax(QKáµ€ / âˆšd) V

---

## Week 9 â€“ Why Attention Exists

### Watch
- CS224n â€“ Sequence Models

### Concepts
- [[RNN Limitations]]
- [[Context Bottleneck]]

Checkpoint:
> I know why transformers replaced RNNs.

---

## Week 10 â€“ Self Attention

### Watch
- CS224n â€“ Attention

### Concepts
- [[Queries]]
- [[Keys]]
- [[Values]]
- [[Scaled Dot Product Attention]]

Checkpoint:
> I can explain each term in QKáµ€.

---

## Week 11 â€“ Transformers

### Watch
- CS224n â€“ Transformers

### Concepts
- [[Multi Head Attention]]
- [[Positional Encoding]]
- [[Encoder Decoder Architecture]]

Checkpoint:
> Transformers are attention + MLP blocks.

---

## Week 12 â€“ Decoder Only Models

### Concepts
- [[Masked Attention]]
- [[Autoregressive Modeling]]

Checkpoint:
> I understand next-token prediction.
