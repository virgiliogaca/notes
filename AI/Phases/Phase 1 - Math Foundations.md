---
tags: [ai, math, foundations]
phase: 1
---

# ðŸŸ¢ Phase 1 â€“ Math Foundations (Weeks 1â€“4)

## Objective

Build intuition for:

- vectors & matrices
- gradients
- probability
- softmax

You should **stop fearing equations** in ML papers.

---

## Week 1 â€“ Linear Algebra

### Read
- [[Mathematics for Machine Learning]] â€“ Linear Algebra
- [[Deep Learning â€“ Goodfellow et al.]] Ch 2 (skim)

### Concepts
- [[Vectors]]
- [[Matrix Multiplication]]
- [[Dot Product]]
- [[Norms]]

### Checkpoint
> I understand why matrix multiplication mixes features.

---

## Week 2 â€“ Calculus for ML

### Read
- MML â€“ Multivariate Calculus

### Focus
- [[Gradients]]
- [[Partial Derivatives]]
- [[Chain Rule]]

Ignore heavy proofs.

### Checkpoint
> I understand how changing weights changes loss.

---

## Week 3 â€“ Probability & Softmax

### Read
- Goodfellow Ch 3

### Concepts
- [[Logits]]
- [[Softmax]]
- [[Entropy]]
- [[Cross Entropy Loss]]

### Checkpoint
> I can explain why softmax uses exp().

---

## Week 4 â€“ Optimization

### Read
- Goodfellow Ch 5 (first half)

### Concepts
- [[Gradient Descent]]
- [[Learning Rate]]
- [[Loss Landscape]]

### Checkpoint
> Training a model is solving an optimization problem.
