---
tags: [ai, neural-networks]
phase: 2
---

# ðŸŸ¡ Phase 2 â€“ Neural Network Theory (Weeks 5â€“8)

## Objective

Understand neural networks **before** transformers.

---

## Week 5 â€“ Feedforward Networks

### Read
- Goodfellow Ch 6

### Concepts
- [[Neural Networks]]
- [[Activation Functions]]
- [[Representation Learning]]

Checkpoint:
> Neural nets are stacked linear layers + nonlinearities.

---

## Week 6 â€“ Backpropagation Intuition

### Focus
- [[Forward Pass]]
- [[Backward Pass]]
- [[Gradient Flow]]

No derivations required.

Checkpoint:
> I understand how error propagates backward.

---

## Week 7 â€“ Optimization in Practice

### Read
- Goodfellow Ch 8

### Concepts
- [[Adam Optimizer]]
- [[SGD]]
- [[Vanishing Gradients]]
- [[Exploding Gradients]]

Checkpoint:
> I understand why normalization stabilizes training.

---

## Week 8 â€“ NLP Foundations

### Watch
- [[Stanford CS224n]] Lecture 1 & 2

### Concepts
- [[Embeddings]]
- [[Vector Similarity]]

Checkpoint:
> Words become points in semantic space.
